{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abba5897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayarawajba/miniconda3/envs/nlp-classification-v01/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29745c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rayarawajba/.cache/kagglehub/datasets/jruvika/fake-news-detection/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"jruvika/fake-news-detection\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7aace8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Introduction:\n",
    "# The following code demonstrates how to process and batch text data for a poem classification task using PyTorch.\n",
    "# It covers loading and preprocessing the dataset, tokenizing and numericalizing the text, creating a custom Dataset,\n",
    "# and batching the data with padding for use in neural network models. This setup is essential for training\n",
    "# deep learning models on variable-length text sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f38a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "                                                URLs  \\\n",
      "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
      "1  https://www.reuters.com/article/us-filmfestiva...   \n",
      "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
      "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
      "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
      "\n",
      "                                            Headline  \\\n",
      "0         Four ways Bob Corker skewered Donald Trump   \n",
      "1  Linklater's war veteran comedy speaks to moder...   \n",
      "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
      "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
      "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
      "\n",
      "                                                Body  Label  \n",
      "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
      "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
      "2  The feud broke into public view last week when...      1  \n",
      "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
      "4  Country singer Jason Aldean, who was performin...      1  \n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset into a DataFrame\n",
    "df = pd.read_csv(path + \"/data.csv\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "print(df.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f731033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4009 entries, 0 to 4008\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   URLs      4009 non-null   object\n",
      " 1   Headline  4009 non-null   object\n",
      " 2   Body      3988 non-null   object\n",
      " 3   Label     4009 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 125.4+ KB\n",
      "None\n",
      "             Label\n",
      "count  4009.000000\n",
      "mean      0.466949\n",
      "std       0.498969\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "Index(['URLs', 'Headline', 'Body', 'Label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Explore the dataset\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b2ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rayarawajba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rayarawajba/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rayarawajba/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rayarawajba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5e6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Check if text is not NaN and is a string\n",
    "        if isinstance(text, str):\n",
    "            # Remove leading and trailing whitespace\n",
    "            text = text.strip()\n",
    "            # Remove HTML tags\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            # Remove email addresses\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "            # Remove non-ASCII characters\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Remove digits\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            # Remove leading and trailing whitespace again\n",
    "            text = text.strip()\n",
    "            # Remove special characters\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Remove single characters\n",
    "            text = re.sub(r'\\b\\w\\b', '', text)\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            # Remove punctuation\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            # Tokenize the text\n",
    "            tokens = word_tokenize(text)\n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "            # Remove numbers\n",
    "            tokens = [word for word in tokens if not word.isdigit()]\n",
    "            # Remove extra spaces\n",
    "            text = ' '.join(tokens)\n",
    "            # Remove special characters\n",
    "            text = re.sub(r'\\W+', ' ', text)\n",
    "        else:\n",
    "            text = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        text = \"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd55fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Sample preprocessed data:\n",
      "0    image copyright getty images sunday morning do...\n",
      "1    london reuters last flag flying comedydrama vi...\n",
      "2    feud broke public view last week mr corker sai...\n",
      "3    mexico city reuters egypt cheiron holdings lim...\n",
      "4    country singer jason aldean performing las veg...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df['Body'] = preprocessed_df['Body'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete. Sample preprocessed data:\")\n",
    "print(preprocessed_df['Body'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b46adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Sample tokens:\n",
      "0    [image, copyright, getty, images, sunday, morn...\n",
      "1    [london, reuters, last, flag, flying, comedydr...\n",
      "2    [feud, broke, public, view, last, week, mr, co...\n",
      "3    [mexico, city, reuters, egypt, cheiron, holdin...\n",
      "4    [country, singer, jason, aldean, performing, l...\n",
      "Name: Tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the preprocessed text\n",
    "preprocessed_df['Tokens'] = preprocessed_df['Body'].apply(word_tokenize)\n",
    "print(\"Tokenization complete. Sample tokens:\")\n",
    "print(preprocessed_df['Tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a552844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in dataset: 1092149\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of tokens in the dataset\n",
    "tokens = []\n",
    "for token_list in preprocessed_df['Tokens']:\n",
    "    tokens.extend(token_list)\n",
    "print(\"Total tokens in dataset:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75beff0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 49205\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = list(set(tokens)) # Sorting here is optional but ensures consistent ID assignment\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "# Create a mapping from tokens to IDs\n",
    "# This mapping will be used to convert tokens to numerical IDs for model input\n",
    "# Ensure that special tokens are included in the mapping\n",
    "word_to_id = {\"<pad>\": PAD_ID, \"<unk>\": UNK_ID} # Special token IDs\n",
    "next_id = 2 \n",
    "\n",
    "for token in unique_tokens:\n",
    "    if token not in word_to_id: # Ensure special tokens are not overwritten if they happen to be in the text\n",
    "        word_to_id[token] = next_id\n",
    "        next_id += 1\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "# print(word_to_id)\n",
    "\n",
    "# id_to_word mapping for debugging/reverse lookup\n",
    "id_to_word = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "685d2079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Tokens  \\\n",
      "0     [image, copyright, getty, images, sunday, morn...   \n",
      "1     [london, reuters, last, flag, flying, comedydr...   \n",
      "2     [feud, broke, public, view, last, week, mr, co...   \n",
      "3     [mexico, city, reuters, egypt, cheiron, holdin...   \n",
      "4     [country, singer, jason, aldean, performing, l...   \n",
      "...                                                 ...   \n",
      "4004  [trends, watch, readers, think, story, fact, a...   \n",
      "4005  [trump, jr, soon, give, minute, speech, reader...   \n",
      "4006                                                 []   \n",
      "4007  [shanghai, reuters, china, said, plans, accept...   \n",
      "4008  [vice, president, mike, pence, leaves, nfl, ga...   \n",
      "\n",
      "                                       Numerical_Tokens  \n",
      "0     [19187, 20100, 19944, 29186, 94, 10804, 37021,...  \n",
      "1     [40821, 28437, 27032, 42569, 39144, 42717, 245...  \n",
      "2     [36544, 39399, 43034, 35529, 27032, 27545, 419...  \n",
      "3     [25052, 8186, 28437, 23970, 13108, 29580, 4691...  \n",
      "4     [7141, 27057, 24447, 1021, 25426, 17825, 3220,...  \n",
      "...                                                 ...  \n",
      "4004  [40464, 6531, 39655, 18494, 38907, 24876, 3186...  \n",
      "4005  [39173, 1650, 25244, 44680, 5295, 21447, 39655...  \n",
      "4006                                                 []  \n",
      "4007  [18767, 28437, 46774, 28883, 44510, 48220, 266...  \n",
      "4008  [41015, 41748, 41594, 37028, 22700, 30711, 442...  \n",
      "\n",
      "[4009 rows x 2 columns]\n",
      "{0: 0, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df['Numerical_Tokens'] = preprocessed_df['Tokens'].apply(\n",
    "    lambda token_list: [word_to_id.get(token, word_to_id[\"<unk>\"]) for token in token_list]\n",
    ")\n",
    "\n",
    "print(preprocessed_df[['Tokens', 'Numerical_Tokens']])\n",
    "\n",
    "#  Numericalize the 'Label' column in the DataFrame ---\n",
    "# Get all unique labels\n",
    "unique_labels = sorted(preprocessed_df['Label'].unique().tolist()) \n",
    "\n",
    "# Create a mapping from string label to integer ID\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "print(label_to_id)\n",
    "# Create a mapping from integer ID to string label\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "num_classes = len(label_to_id)\n",
    "\n",
    "preprocessed_df['Numerical_Label'] = preprocessed_df['Label'].map(label_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae54ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, numerical_tokens, numerical_labels):\n",
    "        self.numerical_tokens = numerical_tokens\n",
    "        self.numerical_labels = numerical_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numerical_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return numerical tokens as a PyTorch tensor and the label\n",
    "        # We convert to tensor here, but padding happens in collate_fn\n",
    "        token_ids = torch.tensor(self.numerical_tokens.iloc[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.numerical_labels.iloc[idx], dtype=torch.long)\n",
    "        return token_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03af1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_required_seq_len = max(model.kernel_sizes)\n",
    "def collate_fn(batch):\n",
    "    # `batch` is a list of tuples, where each tuple is (token_ids, label)\n",
    "    # e.g., [(tensor([3,2,4]), 0), (tensor([5,6,7,8]), 1)]\n",
    "\n",
    "    # Separate token IDs and labels\n",
    "    token_ids_list = [item[0] for item in batch]\n",
    "    labels_list = [item[1] for item in batch]\n",
    "\n",
    "    # Pad the token_ids to the max length in the current batch\n",
    "    # `batch_first=True` means the output tensor will be (batch_size, sequence_length)\n",
    "    padded_token_ids = pad_sequence(token_ids_list,\n",
    "                                    batch_first=True,\n",
    "                                    padding_value=PAD_ID) \n",
    "\n",
    "    # Stack labels into a single tensor\n",
    "    labels = torch.stack(labels_list)\n",
    "\n",
    "    return padded_token_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "838a203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    preprocessed_df,\n",
    "    test_size=0.2,    # 20% for testing\n",
    "    random_state=42,  # A common seed for reproducibility\n",
    "    stratify=preprocessed_df['Label'] # Stratify by the original string label column for balanced splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c9a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df['Numerical_Tokens'], train_df['Numerical_Label'])\n",
    "test_dataset = TextDataset(test_df['Numerical_Tokens'], test_df['Numerical_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30507e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoader created with batch_size=4. Iterating through a few batches:\n",
      "\n",
      "--- Batch 1 ---\n",
      "Padded Token IDs (shape, content):\n",
      "torch.Size([4, 221])\n",
      "tensor([[ 2386, 42569, 38431, 15803, 44974, 34175, 43518, 41755, 19374,  6590,\n",
      "         46658, 28149, 47506, 46552,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [19187, 20100, 11944, 14511, 19187, 32647,  8219, 20618, 21432, 23739,\n",
      "         18758, 20309,  5345, 28714, 39643, 29163, 30638, 27594, 36929, 22941,\n",
      "          2890, 36146,  6960, 23468, 40831, 16062, 25909, 27032, 34350, 46280,\n",
      "           851, 37432, 20702, 29163, 25976,  2535,  8219, 20618, 21888, 13388,\n",
      "         21432, 32663, 39643, 44372, 23468, 40831, 15965, 45851, 24761,  2527,\n",
      "         35689, 15965, 28065, 29163, 40504, 20618, 21432, 10951, 34587, 35037,\n",
      "         32744, 34638, 14891, 29163, 18723,  2395, 12388, 20618, 15065, 16062,\n",
      "         47125, 17284,  9350, 37252, 29658, 34091, 32027, 17428,  6153, 18335,\n",
      "         20702, 20618, 21432, 20220, 18758, 11623, 23237, 14891, 43536,  1447,\n",
      "         24007, 36263,  7155, 48763,  5345, 15585,  5735, 18758, 40544,  5300,\n",
      "         34665, 44386, 25166, 10426, 31103, 10682, 29481, 23108, 28065, 48823,\n",
      "         28883, 23468, 41748, 22476, 11163, 35883,  8219,  4160, 11944, 10963,\n",
      "         46768, 38831, 27275, 32205, 32992, 30562, 47024, 40511, 13775, 32117,\n",
      "         30595,  1915,  5513, 32663, 32928, 10716,  1384, 15965, 45851, 24761,\n",
      "         18006, 31056,  1589, 46458,  3946, 35689, 24614, 16694, 28065, 31056,\n",
      "         29854, 22033,  3946, 10716, 11163,  1384, 39959,  4214, 28106, 39643,\n",
      "         48009, 15109, 12615, 41200, 39284, 28958, 39643, 32062, 31870, 32066,\n",
      "         32717, 28017, 42566, 39643, 41200, 11884, 39643,  1031,  2971, 39959,\n",
      "         18810, 27481, 46828, 32062, 45984, 46280,   851, 47125, 17284,  9350,\n",
      "         10951, 10084, 46117, 31748,  9402, 16694, 22452, 21388,  1675, 47043,\n",
      "         22131, 40806, 18001, 14520,  2209, 12388, 36911, 17374,   424, 47024,\n",
      "         16694, 38421, 17374, 44372, 23468, 10951, 23815, 40831, 21432, 37252,\n",
      "         10115],\n",
      "        [23046, 34893, 15824, 39904, 39173, 29114, 32066, 33793, 33852, 32663,\n",
      "         21434,  5726, 14947,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [30595, 46619, 24275,  6795, 15065, 28535, 25430, 18569, 39655, 18494,\n",
      "         38907, 24876, 31864, 10084, 46762,  2204, 30140, 41755,  7364, 29547,\n",
      "          6205, 48168, 16047, 47355,  1809, 29658,  8522,    19, 22966, 29278,\n",
      "         11665,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]])\n",
      "Labels (shape, content):\n",
      "torch.Size([4])\n",
      "tensor([0, 1, 0, 0])\n",
      "\n",
      "--- Batch 2 ---\n",
      "Padded Token IDs (shape, content):\n",
      "torch.Size([4, 547])\n",
      "tensor([[ 9619,  8813,  4868,  ...,     0,     0,     0],\n",
      "        [ 8158, 23451, 44536,  ...,     0,     0,     0],\n",
      "        [11855,  9676, 13926,  ..., 22668,  6071, 28883],\n",
      "        [31304, 26714, 29636,  ...,     0,     0,     0]])\n",
      "Labels (shape, content):\n",
      "torch.Size([4])\n",
      "tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # Choose your batch size\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, # Shuffle for training\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nDataLoader created with batch_size={batch_size}. Iterating through a few batches:\")\n",
    "\n",
    "# --- 4. Iterate through the DataLoader to see the padded batches ---\n",
    "for i, (batch_tokens, batch_labels) in enumerate(train_dataloader):\n",
    "    print(f\"\\n--- Batch {i+1} ---\")\n",
    "    print(\"Padded Token IDs (shape, content):\")\n",
    "    print(batch_tokens.shape)\n",
    "    print(batch_tokens)\n",
    "    print(\"Labels (shape, content):\")\n",
    "    print(batch_labels.shape)\n",
    "    print(batch_labels)\n",
    "\n",
    "    if i >= 1: # Just show a couple of batches\n",
    "        break\n",
    "\n",
    "# This `batch_tokens` tensor (e.g., shape [batch_size, max_seq_len_in_batch])\n",
    "# is what you directly feed into your PyTorch nn.Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4de7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, pad_idx):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        # pad_idx tells the embedding layer to not update the embedding for this index (PAD_ID)\n",
    "        # and it will output zeros for that index.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # 2. Convolutional Layers (usually multiple filters with different kernel sizes)\n",
    "        # These capture n-gram features of different lengths\n",
    "        self.kernel_sizes = [3, 4, 5] # Example: capture 3-gram, 4-gram, 5-gram features\n",
    "        self.num_filters = 100        # Number of filters (feature detectors) per kernel size\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim, # Input channels are the embedding dimension\n",
    "                      out_channels=self.num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # 3. Fully Connected (Dense) Layer for classification\n",
    "        # Sum of num_filters for each kernel size, as we concatenate their outputs\n",
    "        self.fc = nn.Linear(len(self.kernel_sizes) * self.num_filters, num_classes)\n",
    "\n",
    "        # Dropout for regularization (to prevent overfitting)\n",
    "        self.dropout = nn.Dropout(0.5) # Example dropout rate\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, sequence_length)\n",
    "\n",
    "        # Pass through embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # PyTorch Conv1d expects input in (batch_size, channels, sequence_length)\n",
    "        # So we permute the dimensions\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        # embedded shape: (batch_size, embedding_dim, sequence_length)\n",
    "\n",
    "        # Apply convolutions and ReLU activation\n",
    "        # For each conv layer, apply it, then apply ReLU, then apply global max pooling\n",
    "        # The pooling operation extracts the most important feature from each filter's output\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conved[i] shape: (batch_size, num_filters, output_sequence_length)\n",
    "\n",
    "        # Apply global max pooling over the sequence dimension\n",
    "        # This takes the maximum value from each filter's output across the entire sequence\n",
    "        pooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in conved]\n",
    "        # pooled[i] shape: (batch_size, num_filters)\n",
    "\n",
    "        # Concatenate the pooled outputs from all kernel sizes\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat shape: (batch_size, num_filters * len(kernel_sizes))\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(cat)\n",
    "        # output shape: (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81d7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "embedding_dim = 100 \n",
    "model = TextCNN(vocab_size, embedding_dim, num_classes, PAD_ID)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1632b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train() # Set model to training mode\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d61856da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.575658 [    4/ 3207]\n",
      "loss: 0.289116 [  404/ 3207]\n",
      "loss: 0.500863 [  804/ 3207]\n",
      "loss: 1.317698 [ 1204/ 3207]\n",
      "loss: 0.140407 [ 1604/ 3207]\n",
      "loss: 0.212157 [ 2004/ 3207]\n",
      "loss: 0.678218 [ 2404/ 3207]\n",
      "loss: 4.055592 [ 2804/ 3207]\n",
      "loss: 0.002178 [ 3204/ 3207]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.183415 [    4/ 3207]\n",
      "loss: 0.000081 [  404/ 3207]\n",
      "loss: 2.028860 [  804/ 3207]\n",
      "loss: 0.008639 [ 1204/ 3207]\n",
      "loss: 0.233213 [ 1604/ 3207]\n",
      "loss: 0.770672 [ 2004/ 3207]\n",
      "loss: 0.003216 [ 2404/ 3207]\n",
      "loss: 0.010467 [ 2804/ 3207]\n",
      "loss: 0.591287 [ 3204/ 3207]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.396940 [    4/ 3207]\n",
      "loss: 0.358429 [  404/ 3207]\n",
      "loss: 0.019943 [  804/ 3207]\n",
      "loss: 0.647841 [ 1204/ 3207]\n",
      "loss: 0.000765 [ 1604/ 3207]\n",
      "loss: 0.005887 [ 2004/ 3207]\n",
      "loss: 0.001492 [ 2404/ 3207]\n",
      "loss: 4.329236 [ 2804/ 3207]\n",
      "loss: 0.000229 [ 3204/ 3207]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.681771 [    4/ 3207]\n",
      "loss: 0.016275 [  404/ 3207]\n",
      "loss: 3.219456 [  804/ 3207]\n",
      "loss: 0.000000 [ 1204/ 3207]\n",
      "loss: 0.002526 [ 1604/ 3207]\n",
      "loss: 0.000002 [ 2004/ 3207]\n",
      "loss: 0.424796 [ 2404/ 3207]\n",
      "loss: 1.915560 [ 2804/ 3207]\n",
      "loss: 0.018823 [ 3204/ 3207]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.168355 [    4/ 3207]\n",
      "loss: 1.829388 [  404/ 3207]\n",
      "loss: 0.000015 [  804/ 3207]\n",
      "loss: 0.000093 [ 1204/ 3207]\n",
      "loss: 0.679073 [ 1604/ 3207]\n",
      "loss: 0.000173 [ 2004/ 3207]\n",
      "loss: 0.000000 [ 2404/ 3207]\n",
      "loss: 4.750213 [ 2804/ 3207]\n",
      "loss: 0.000000 [ 3204/ 3207]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 # Set the number of epochs for training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5599ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # No gradient calculation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in dataloader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_tokens)\n",
    "\n",
    "            # Calculate loss (optional for testing, but good for understanding)\n",
    "            loss_fn = nn.CrossEntropyLoss() # Use the same loss function as training\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions (the class with the highest probability/logit)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Generate a classification report (precision, recall, f1-score per class)\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(label_to_id.keys()), output_dict=True)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy, report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c75b0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results on Training Dataset ---\n",
      "Test Loss: 0.0810\n",
      "Test Accuracy: 0.9738\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.997551  0.953189  0.974865  1709.000000\n",
      "1              0.949174  0.997330  0.972656  1498.000000\n",
      "accuracy       0.973807  0.973807  0.973807     0.973807\n",
      "macro avg      0.973362  0.975259  0.973761  3207.000000\n",
      "weighted avg   0.974954  0.973807  0.973833  3207.000000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1629   80]\n",
      " [   4 1494]]\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "train_loss, train_accuracy, train_report, train_confusion_matrix = evaluate_model(model, train_dataloader, device=torch.device('cpu'))\n",
    "\n",
    "print(f\"\\n--- Test Results on Training Dataset ---\")\n",
    "print(f\"Test Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {train_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "# Convert dict to string for pretty print\n",
    "print(pd.DataFrame(train_report).transpose())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(train_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34e3c332",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# The following code demonstrates how to evaluate a trained TextCNN model on a test dataset using PyTorch.\n",
    "# It sets up a DataLoader for batching and padding, runs the evaluation loop, and prints out key metrics\n",
    "# such as test loss, accuracy, classification report, and confusion matrix. This process helps assess\n",
    "# the model's performance on unseen data and provides insights into its predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f92d761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader ready.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate your dataloader for the test dataset\n",
    "test_batch_size = 8 \n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "\n",
    "print(\"Test DataLoader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ddaa186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Results ---\n",
      "Test Loss: 0.1926\n",
      "Test Accuracy: 0.9514\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score     support\n",
      "0              0.997442  0.911215  0.952381  428.000000\n",
      "1              0.907543  0.997326  0.950318  374.000000\n",
      "accuracy       0.951372  0.951372  0.951372    0.951372\n",
      "macro avg      0.952493  0.954271  0.951350  802.000000\n",
      "weighted avg   0.955519  0.951372  0.951419  802.000000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[390  38]\n",
      " [  1 373]]\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation\n",
    "test_loss, test_accuracy, test_report, test_confusion_matrix = evaluate_model(model, test_dataloader, device=torch.device('cpu'))\n",
    "\n",
    "print(f\"\\n--- Test Results ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "# Convert dict to string for pretty print\n",
    "print(pd.DataFrame(test_report).transpose())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "022c621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to textcnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Export the trained model to a file\n",
    "torch.save(model.state_dict(), \"textcnn_model.pth\")\n",
    "print(\"Model exported to textcnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70f2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the word_to_id mapping\n",
    "with open('word_to_id.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_id, f)\n",
    "# Save the id_to_label mapping\n",
    "with open('id_to_label.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_label, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-classification-v01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
