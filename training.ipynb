{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29745c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"muhammadimran112233/liar-twitter-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7aace8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# Introduction:\n",
    "# The following code demonstrates how to process and batch text data for news classification task using PyTorch.\n",
    "# It covers loading and preprocessing the dataset, tokenizing and numericalizing the text, creating a custom Dataset,\n",
    "# and batching the data with padding for use in neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f38a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset into a DataFrame\n",
    "df = pd.read_csv(path + \"/Liar_Dataset.csv\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "print(df.head())  # Display the first few rows of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    try:\n",
    "        # Check if text is not NaN and is a string\n",
    "        if isinstance(text, str):\n",
    "            # Remove leading and trailing whitespace\n",
    "            text = text.strip()\n",
    "            # Remove HTML tags\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "            # Remove email addresses\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "            # Remove non-ASCII characters\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Remove digits\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            # Remove leading and trailing whitespace again\n",
    "            text = text.strip()\n",
    "            # Remove special characters\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            # Remove extra spaces\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            # Remove single characters\n",
    "            text = re.sub(r'\\b\\w\\b', '', text)\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "            # Remove punctuation\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            # Tokenize the text\n",
    "            tokens = word_tokenize(text)\n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "            # Remove numbers\n",
    "            tokens = [word for word in tokens if not word.isdigit()]\n",
    "            # Remove extra spaces\n",
    "            text = ' '.join(tokens)\n",
    "            # Remove special characters\n",
    "            text = re.sub(r'\\W+', ' ', text)\n",
    "        else:\n",
    "            text = \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        text = \"\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd55fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df['statement'] = preprocessed_df['statement'].apply(preprocess_text)\n",
    "print(\"Preprocessing complete. Sample preprocessed data:\")\n",
    "print(preprocessed_df['statement'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b46adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text\n",
    "preprocessed_df['Tokens'] = preprocessed_df['statement'].apply(word_tokenize)\n",
    "print(\"Tokenization complete. Sample tokens:\")\n",
    "print(preprocessed_df['Tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a552844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of tokens in the dataset\n",
    "tokens = []\n",
    "for token_list in preprocessed_df['Tokens']:\n",
    "    tokens.extend(token_list)\n",
    "print(\"Total tokens in dataset:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75beff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(set(tokens))\n",
    "PAD_ID = 0\n",
    "UNK_ID = 1\n",
    "# Create a mapping from tokens to IDs\n",
    "# This mapping will be used to convert tokens to numerical IDs for model input\n",
    "# Ensure that special tokens are included in the mapping\n",
    "word_to_id = {\"<pad>\": PAD_ID, \"<unk>\": UNK_ID} # Special token IDs\n",
    "next_id = 2 \n",
    "\n",
    "for token in unique_tokens:\n",
    "    if token not in word_to_id: # Ensure special tokens are not overwritten if they happen to be in the text\n",
    "        word_to_id[token] = next_id\n",
    "        next_id += 1\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "# print(word_to_id)\n",
    "\n",
    "# id_to_word mapping for debugging/reverse lookup\n",
    "id_to_word = {v: k for k, v in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df['Numerical_Tokens'] = preprocessed_df['Tokens'].apply(\n",
    "    lambda token_list: [word_to_id.get(token, word_to_id[\"<unk>\"]) for token in token_list]\n",
    ")\n",
    "\n",
    "# Numericalization of the 'label' column\n",
    "# Get all unique labels\n",
    "unique_labels = sorted(preprocessed_df['label'].unique().tolist()) \n",
    "\n",
    "# Create a mapping from string label to integer ID\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "print(label_to_id)\n",
    "# Create a mapping from integer ID to string label\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "num_classes = len(label_to_id)\n",
    "\n",
    "preprocessed_df['Numerical_Label'] = preprocessed_df['label'].map(label_to_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, numerical_tokens, numerical_labels):\n",
    "        self.numerical_tokens = numerical_tokens\n",
    "        self.numerical_labels = numerical_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numerical_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return numerical tokens as a PyTorch tensor and the label\n",
    "        # We convert to tensor here, but padding happens in collate_fn\n",
    "        token_ids = torch.tensor(self.numerical_tokens.iloc[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.numerical_labels.iloc[idx], dtype=torch.long)\n",
    "        return token_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = [3, 4, 5]  # Example kernel sizes for CNN\n",
    "min_required_seq_len = max(kernel_sizes)  # Define this based on your model's kernel sizes\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # Separate token IDs and labels\n",
    "    token_ids_list = [item[0] for item in batch]\n",
    "    labels_list = [item[1] for item in batch]\n",
    "\n",
    "    # Ensure each token_ids is at least min_required_seq_len long\n",
    "    adjusted_token_ids_list = []\n",
    "    for seq in token_ids_list:\n",
    "        if len(seq) < min_required_seq_len:\n",
    "            pad = torch.full((min_required_seq_len - len(seq),), PAD_ID, dtype=seq.dtype)\n",
    "            seq = torch.cat([seq, pad], dim=0)\n",
    "        adjusted_token_ids_list.append(seq)\n",
    "\n",
    "    # Pad all to batch max length (could be > min_required_seq_len!)\n",
    "    padded_token_ids = pad_sequence(adjusted_token_ids_list,\n",
    "                                   batch_first=True,\n",
    "                                   padding_value=PAD_ID) \n",
    "\n",
    "    # Stack labels into a single tensor\n",
    "    labels = torch.stack(labels_list)\n",
    "\n",
    "    return padded_token_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    preprocessed_df,\n",
    "    test_size=0.2,    # 20% for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=preprocessed_df['label'] # Stratify by the original string label column for balanced splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df['Numerical_Tokens'], train_df['Numerical_Label'])\n",
    "test_dataset = TextDataset(test_df['Numerical_Tokens'], test_df['Numerical_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30507e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "print(f\"\\nDataLoader created with batch_size={batch_size}. Iterating through a few batches:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4de7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, pad_idx):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        #  Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        #  Convolutional Layers \n",
    "        self.kernel_sizes = [3, 4, 5] \n",
    "        self.num_filters = 100 e\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=self.num_filters,\n",
    "                      kernel_size=k)\n",
    "            for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        #  Fully Connected (Dense) Layer for classification\n",
    "        self.fc = nn.Linear(len(self.kernel_sizes) * self.num_filters, num_classes)\n",
    "\n",
    "        # Dropout for regularization \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: (batch_size, sequence_length)\n",
    "\n",
    "        # Pass through embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded shape: (batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # PyTorch Conv1d expects input in (batch_size, channels, sequence_length)\n",
    "        # So we permute the dimensions\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "\n",
    "        # Apply convolutions and ReLU activation\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "\n",
    "        # Apply global max pooling over the sequence dimension\n",
    "        pooled = [F.max_pool1d(c, c.shape[2]).squeeze(2) for c in conved]\n",
    "\n",
    "        # Concatenate the pooled outputs from all kernel sizes\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat shape: (batch_size, num_filters * len(kernel_sizes))\n",
    "\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(cat)\n",
    "        # output shape: (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "embedding_dim = 50 \n",
    "model = TextCNN(vocab_size, embedding_dim, num_classes, PAD_ID)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61856da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 20 \n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # No gradient calculation needed during evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch_tokens, batch_labels in dataloader:\n",
    "            batch_tokens = batch_tokens.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_tokens)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    # Generate a classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=list(label_to_id.keys()), output_dict=True)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy, report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "train_loss, train_accuracy, train_report, train_confusion_matrix = evaluate_model(model, train_dataloader, device=torch.device('cpu'))\n",
    "\n",
    "print(f\"\\n--- Test Results on Training Dataset ---\")\n",
    "print(f\"Test Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {train_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "print(pd.DataFrame(train_report).transpose())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(train_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3c332",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# The following code demonstrates how to evaluate a trained TextCNN model on a test dataset using PyTorch.\n",
    "# It sets up a DataLoader for batching and padding, runs the evaluation loop, and prints out key metrics\n",
    "# such as test loss, accuracy, classification report, and confusion matrix. This process helps assess\n",
    "# the model's performance on unseen data and provides insights into its predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dataloader for the test dataset\n",
    "test_batch_size = 8 \n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=test_batch_size,\n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "\n",
    "print(\"Test DataLoader ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddaa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "test_loss, test_accuracy, test_report, test_confusion_matrix = evaluate_model(model, test_dataloader, device=torch.device('cpu'))\n",
    "\n",
    "print(f\"\\n--- Test Results ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "print(pd.DataFrame(test_report).transpose())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3482b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'num_classes': num_classes,\n",
    "    'pad_idx': PAD_ID,  \n",
    "    'unk_idx': UNK_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the trained model to a file\n",
    "torch.save(model.state_dict(), \"textcnn_model.pth\")\n",
    "print(\"Model exported to textcnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the word_to_id mapping\n",
    "with open('word_to_id.pkl', 'wb') as f:\n",
    "    pickle.dump(word_to_id, f)\n",
    "# Save the id_to_label mapping\n",
    "with open('id_to_label.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_label, f)\n",
    "with open('model_args.pkl', 'wb') as fp:\n",
    "    pickle.dump(model_args, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-classification-v01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
